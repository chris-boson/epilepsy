\section{Introduction to Natural Language Processing}

Natural language processing (NLP) is a field of computer science that deals with the extraction, processing, and understanding of human language.
It is known as the field of computer linguistics, and is a subfield of artificial intelligence.
Common NLP tasks include sentence segmentation, tokenization, part-of-speech tagging, named-entity recognition, parsing, question answering, summarization and classification.

How can we teach a computer to perform these tasks?
The first challenge is that computers, at their core, only understand numbers.
So first we need to think about how words, or more generally text, can be represented with numbers such that we can perform calculations on them.
The numbers should allow the computer to assign meaning to words and their context with other words around it.

There is no perfect way to represent language in this numeric fashion. The best we can do is to capture representations that we feel capture the \textit{syntactic} features of text, as well as \textit{semantic} features of text.
Syntactic information refers to grammatical constructs and the way that words are put together in a language. Semantic information refers to the meaning of this body of text. Such distinctions are fundamental concepts in computer science
and date back to the very foundations of information theory. \cite{shannon48}
Much of this chapter, as well as much of the current work in the field of computational linguistics is centered around finding better and better representations that capture both types of this information.

How do we begin to choose these representations?
Do we choose to represent words, or perhaps individual letters?
Do we choose all the words?
What do we do with punctuation and numbers?

\subsection{Vocabularies and Words}
\label{vocabularies_and_words}
In NLP, the set of unique words in a corpus is called a \textit{vocabulary}. While \textit{the} is the most common word in the English language, it is only one entry in a very large vocabulary table. A rare word like \textit{hemispherectomy} is also one
entry in this table. Both \textit{apple} and \textit{apples} might be separate entries in our vocabulary table. Generally the first step in many NLP systems is defining this vocabulary and the rules behind which words are in this table and which words are not.

The process of splitting text into separate smaller units, in this case words, is known as \textit{tokenization}.
When we run many NLP tasks, we almost always preprocess our text by putting it into one of these tokenizers.
The output tokens, in this case words, are the indexes in our lookup table to different numeric representations that can be understood by a machine.
There are a number of great tokenization libraries that are open source, and you should always use them rather than try to implement this yourself.

\begin{python}
  # Example 1: Spacy Tokenization Using Medical Words

  import spacy
  sample = "After a temporal lobe resection, the " \
           "atonic and clonic seizure frequency " \
           "fell by 50%."

  nlp = spacy.load("en_core_web_sm")
  english_vocab = set(nlp.vocab.strings)
  spacy_document = nlp(sample)

  for token in spacy_document:
      found = token.text in english_vocab

      print("{}: {}".format(token.text, found))
\end{python}

The discrete numeric entities created in Example 1 are the indexes for our representations.
The tokenizer system did much of the magic under the hood, lowercasing and standardizing text and creating special tokens for punctuation and numbers.

When we build a vocabulary $V$, we define a preset vocabulary size $|V|$. There is a tradeoff in your choice of $|V|$.
If you choose a number that is too small, then your system will only recognize a few words and lose a lot of important task-specific vocaublary.
However, if you include the entire english language, your system will be slow, expensive, and not perform as well.
Commonly this number is set around 20,000 - 40,000 words in English, though it can be much larger.

Often a vocabulary will include special tokens that make our lives easier when working with NLP tasks.
Common special tokens include:
\begin{itemize}
  \item \pythoninline{[EOS]} - An end of sentence/input marker.
  \item \pythoninline{[PAD]} - Special inputs to ignore, usually following an EOS marker.
  \item \pythoninline{[SEP]} - A separator, which can be used in inputs that contain multiple sentences or samples.
  \item \pythoninline{[OOV]} - Out of vocabulary, also commonly represented as [UNK] (unknown), which is used when we come across a word that does not exist in our vocabulary.
\end{itemize}

The out-of-vocabulary token is of particular importance in medicine. If we load a vocabulary with 30,000 words, often these are roughly the most common 30,000 words in the English language. We will still have a few [OOV] tokens
for rare words. However, just because a general system chooses its vocabulary based on word frequency does not mean that this is the best choice of vocabulary for the task at hand. Medical corpora have a very different word frequency
distribution than non-medical corpora, particulary because of nuanced vocabulary that is absolutely crucial.

%TODO - add some epilepsy-themed example that throws away all the epilepsy words%

In Example 1 above, notice the importance of the words that have been thrown away. If this sentence were to be loaded into a machine as-is, we might be losing far too much information because we have to nearly all of the relevant terminology with OOV tokens.
We will address this issue in the ensuing sections. For now, suffice it to say that we should make sure our vocabularies are catered toward our task at hand or are built in such a way that they can still extract signal from these tokens. Furthermore, we should always sanity check our sample token outputs against our raw text inputs.

\subsection{Word Representations}

Given that we have defined a vocabulary $V$ of words, these are the keys to our lookup table of numeric representations. But what do we store in that table as the value?

A simple way to achieve this representation for $V$ is to assign each word a unique integer $i$.
The word is then represented as a vector $w$ of length $|V|$ with all zeros and a one\footnote{This is also known as a one-hot encoding}. at index $i$.

\begin{python}
  have = [1, 0, 0, 0, 0, 0, ... 0]
  a    = [0, 1, 0, 0, 0, 0, ... 0]
  good = [0, 0, 1, 0, 0, 0, ... 0]
  day  = [0, 0, 0, 1, 0, 0, ... 0]
  ...
\end{python}

We could now represent a sentence as the sum of the word vectors $S = \sum_j w_j$. This representation is suitable as input for any classification algorithm (such as logistic regression or a decision tree), to make a prediction about our target variable, e.g. whether the sentence is relevant to our epilepsy task.
This simple representation fulfills our requirements but comes with some drawbacks:
\begin{itemize}
    \item We implicitly assume that each word in the sentence is equally important.
    \item Each word is equally similar to every other word (e.g. by taking the Euclidean distance between word vectors).
    \item The representation is invariant to a reordering of the words.
\end{itemize}
The first point is a problem, because as we add more word vectors together, the sentence representation will converge to the global average and drown out any signal relevant to the specific sentence at hand.
To address this we can instead write a weighted sum $S = \sum_j \lambda_j w_j$, where each word vector is weighted by its importance $\lambda_j$. There are statistical methods we can use to compute an importance value (e.g. TF-IDF).
For example a word like \textit{the} is very common and appears in many different contexts.
It is unlikely that there is a lot of signal we can extract from it.
On the other hand, a word like \textit{epilepsy} will be much more rare and specific to a given task, and we would like to raise its importance.
This has the net effect of allowing us to find good representations for larger word sequences.
To address the second point, we will give a brief introduction into Deep Learning (DL) and the extremely powerful concept of \textit{Embeddings}, one that is not only relevant in NLP, but in the world of AI in general.
