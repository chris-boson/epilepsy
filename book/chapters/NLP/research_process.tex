
\section{Research Process}
Part of any research process is rigorous record keeping to ensure experiments can be independently and reliably reproduced.
While researchers keep meticulous journals, bookkeeping can be largely automated when dealing with software.


\subsection{Data Preparation}
We will go over each step of data preparation in the next sections: Data download, pre-processing, splitting and efficiently feeding data to the model during training.

The goal is to rerun an experiment from scratch with a single command.
We build on top of PyTorch Lightning's \pythoninline{DataModule} class to handle these steps.

\subsubsection{Data Download}
Downloading the dataset from cloud storage and caching are handled in the \pythoninline{prepare_data} method and only executed once per run.
We will first check if the data is already cached, and if not, download it.

\subsubsection{Data Preprocessing}
Pre-processing refers to transformation steps we want to apply to the raw data to make it suitable to input into the model.
For example in NLP, raw text needs to be cleaned, tokenized, trimmed and padded (see Section \ref{vocabularies_and_words}) (Fig. \ref{fig:tokenization}).
\begin{figure}
    \includegraphics[width=\linewidth]{chapters/NLP/figures/tokenization.png}
    \caption{Tokenization}
    \label{fig:tokenization}
\end{figure}
These steps can be slow and in principle we need to do them only once per sample and cache the results.

\subsubsection{Data Splitting}
\begin{figure}[h]
    \includegraphics[width=\linewidth]{chapters/NLP/figures/data_splitting.png}
    \caption{Data Splitting}
    \label{fig:data_splitting}
\end{figure}
To evaluate the generalization performance of a model we need to test it on data it hasn't seen during training.
The dataset is therefore split into training and validation sets, usually at random (Fig. \ref{fig:data_splitting}).
It should be ensured the randomness is deterministic to reproduce results exactly, which can be achieved by setting a "seed" for the random number generator.
The validation set is used during training to monitor progress and tune hyperparameters.
In addition, it is good practice to hold out another portion of the data, the test set, which is never used during training, but only to report final results.
We otherwise run the risk of tuning hyperparameters to the validation data, thereby overestimating real performance.
While datasets are continuously evolving, we should keep a frozen version of the data to get fair comparisons.

\begin{itemize}
    \item Training dataset: Tune model parameters $\theta$ (80\% of the data)
    \item Validation dataset: Tune hyperparameters (10\% of the data)
    \item Testing dataset: Evaluate and report the performance of the model (10\% of the data)
\end{itemize}


\subsubsection{Feeding Data to the Model}
\begin{figure}[h]
    \includegraphics[width=\linewidth]{chapters/NLP/figures/ram_cpu_vram.png}
    \caption{Data needs to be moved from disk to RAM to VRAM so that it can be used to update the model parameters.}
    \label{fig:ram_cpu_vram}
\end{figure}
Model parameters are stored in GPU memory (known as VRAM); in order to update them, the GPU needs to process data from the training set.
After downloading the data from cloud storage it resides on local disk, from where it needs to be moved to RAM and finally VRAM.
Loading data into the model needs to be fast, or we run the risk of starving the GPU, meaning the GPU processes each batch of data faster than the CPU can deliver the next one, leading to low resource utilization and slow training.
We use PyTorch's \pythoninline{DataLoader} class to parallelize this process across multiple CPU workers.
A \pythoninline{DataLoader} takes as argument a \pythoninline{Dataset} and provides a stream of batches of data.
The \textit{batch size} is an important parameter that needs to be tuned on a case by case basis, as we will explain in the next section.

\subsection{Training}
\begin{figure}[h]
    \includegraphics[width=\linewidth]{chapters/NLP/figures/model_architecture.png}
    \caption{Deep neural network architecture}
    \label{fig:model_architecture}
\end{figure}

While glossing over some details, model training can summarized in the following steps:
\begin{itemize}
    \item Given data pairs $(x_i, y_i)$, where $x_i$ is an input sample, and $y_i$ is the desired output
    \item Determine a function $f$, such that $f(\theta, x_i) = \hat{y}_i \approx y_i$, where $\theta$ are model parameters and $f$ is the model architecture
    \item Define a loss function $L(\theta) = \sum_i L(\hat{y}_i, y_i)$ that measures how close the model's output is to the desired output
    \item Optimize $\theta$ to minimize $L(\theta)$
\end{itemize}
\begin{figure}[h]
    \includegraphics[width=\linewidth]{chapters/NLP/figures/loss.png}
    \caption{Minimizing the loss function}
    \label{fig:loss}
\end{figure}
Here the loss function can take on many forms, depending on the training task, we summarize some of the most common ones in Table \ref{table:losses}.
\begin{table}
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{|c| c| c|}
    Loss Name & Function & Use \\[0.5ex] \hline
    Mean Squared Error & $\begin{array} {lcl} \frac{1}{N} \sum_{i=1}^N|\hat{y}_i - y_i|^2\end{array}$  & regression \\ [0.5ex]
    Binary Cross Entropy & $\begin{array} {r@{}l@{}} -\frac{1}{N} \sum_{i=1}^N (y_i \cdot log(\hat{y}_i) + (1 - y_i) \cdot log(1-\hat{y}_i)) \end{array}$ & binary classification \\ [0.5ex]
    Cross Entropy & $\begin{array} {r@{}l@{}} -\frac{1}{N} \sum_{i=1}^N y_i \cdot log(\hat{y}_i) \end{array}$ & multiclass classification \\ [0.5ex]
    \end{tabular}
    \caption{Loss functions}
    \label{table:losses}
\end{table}
Parameters in the last layer are then optimized according to the rule
\begin{equation}
    \label{eq:optimization}
    \theta \rightarrow \theta - \alpha \frac{\partial L}{\partial \theta}
\end{equation}
where $\alpha$ is called the \textit{learning rate} and determines the step size, the partial derivative of the loss function with respect to the parameters is called the \textit{gradient}.
For other parameters the rule is similar, but involves the application of the chain rule from calculus.
The algorithm is called \textit{gradient descent} and is the workhorse of all modern deep learning.
In contemporary architectures there are millions to hundreds of billions of parameters, and updating them efficiently is crucial.

To optimize the parameters as outlined in Eq. \ref{eq:optimization} we need to evaluate the gradient of the loss function given the data $(x_i, y_i)$.
To compute the optimial update step we need to sum over all samples in the dataset, compute and store all the gradients, update the parameters once, and repeat the process until convergence.
This, however is not practical, given the size of typical datasets and the number of model parameters -- each step would be very computationally expensive and the training slow.

On the other end of the spectrum we could use only a single sample to compute approximations of the optimal gradients, and they will take us close to the global minimum.
This method is called \textit{stochastic gradient descent} and generally works well, but it comes with some tradeoffs:
\begin{itemize}
    \item Loading single samples from the dataset is slow, due to CPU, RAM and bus overhead when copying data to VRAM
    \item There is overhead in the GPU related to scheduling of compute operations
    \item The gradients are much more noisy than the optimal ones, and the model will learn more slowly (see Fig. \ref{fig:mini-batch})
\end{itemize}
There is a happy middle ground called \textit{mini-batch gradient descent} which is a combination of stochastic gradient descent with mini-batches.
Using mini-batches allows for a more accurate estimate of the gradient, and reduces the overhead per sample, as we can take advantage of hardware accelerated vectorized compute operations in CPUs and GPUs.
In practice, we will choose a batch size that is much smaller than the number of samples in the dataset, and as big as possible without running out of VRAM, where the latter condition is typically the more relevant constraint.

\begin{figure}[h]
    \includegraphics[width=\linewidth]{chapters/NLP/figures/mini-batch.png}
    \caption{Batch vs mini-batch vs stochastic gradient descent}
    \label{fig:mini-batch}
\end{figure}

\subsection{Over- and Underfitting}
\begin{figure}[h]
    \centering
    \includegraphics[width=1.2\textwidth]{chapters/NLP/figures/over_and_underfitting.png}
    \caption{Left: Underfitting, a straight line doesn't fit  the data well.
    Right: Overfitting, a complex curve fits the (training) data perfectly, but will likely not generalize well.
    Center: The model and true underlying function match closely.}
    \label{fig:over_and_underfitting}
\end{figure}
A common problem in ML is the balance between over- and underfitting (Figure \ref{fig:over_and_underfitting}).
Underfitting can happen when the model is too simple, i.e. it is not able to capture the underlying function (rarely a problem in deep learning).
Overfitting happens when the model is too complex or has too many degrees of freedom, such that it can fit the training data well, but does not capture the essence of the true data generating process, leading to poor generalization performance when applied to new data.

An easy way to detect these issues is by observing training and validation metrics as a function training steps or epochs, called learning curves.
We want to see that both, training and validation metrics continue to improve as training progresses until they both taper off at a certain point.
Note that it is normal to see a difference in the absolute value and rate of change between the training and validation metrics.
This is called the generalization error, and is nothing to be too concerned about, ultimately only the validation and test metrics are relevant.
What \textit{is} problematic is if we observe that the validation metrics start to get worse, while the training metrics continue to improve -- this is a clear sign of overfitting and we should stop the training early (Figure \ref{fig:learning_curves}).
\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{chapters/NLP/figures/learning_curves.png}
    \caption{Learning curves, the training and validation loss are plotted as a function of the number of training steps / epochs.}
    \label{fig:learning_curves}
\end{figure}
To address underfitting, the easiest remedy is to increase the complexity of the model (e.g. add more layers) or to increase the number of training steps.
We can also try to add more features to the dataset, such as adding more words to the vocabulary.
When dealing with overfitting in deep learning, we usually don't want to reduce the model complexity, but instead add \textit{regularization} and more data.
Regularization is a technique that restricts the freedom of the model, for example by introducing a penalty on the magnitude of the model parameters.
For this, the loss functions in Table \ref{table:losses} are modified as
\begin{equation}
    \tilde{L}(\theta) = L(\theta) + \frac{1}{2} \sum_{i} |\theta_i|^2.
\end{equation}

Another common technique is to add \textit{dropout}, where a fraction of neurons is randomly turned off for each training step.
This ensures that each neuron has a unique purpose and learns to detect a specific set of features in the data, without co-adapting to the output of other neurons.
Another interpretation of dropout is that it can be seen as an approximation to a bagging technique, where the outputs of different models at each iteration are averaged, leading to an overall reduction in errors if they are uncorrelated.
This tends to reduce generalization error, prevent the model from fitting to irrelevant details, and make it more robust to noise.

% NOTE - \subsection{Data Sharing for ML (include somewhere here?)}
