\section{NLP Tasks in Epilepsy}

Now that we have an understanding of some of the basics of computational representations of language data, let us take a general look at some of the modern use cases for NLP in industry, 
catered toward epileptologists and other medical practitioners. In this section We will briefly cover examples of that all follow a common pattern. Once we establish this pattern, the
following sections will take a deeper look with working examples of Financial Impact Analysis and SUDEP prediction.

\subsection{Unstructured Text - Retrospective Research}
Within the medical field, there are many sources of unstructured text that are primarily aimed as a either a communication channel between specialists, or a way for patients to pass information that isn't captured in a structured format.
Clinical Notes, Progress Notes, Operative Notes, Discharge Summaries, Pathology Reports, Surgery Reports; all of these sources carry information, but traditionally require experts in the field to extract it.
If we can teach systems to read through this unstructured text and perform respectably close to these experts, we can perform large-scale clinical retrospective research much faster and at a much lower cost.

Because these pre-trained language models can draw upon information from so many domains and have a sound understanding of language, they need relatively few
labeled examples to begin to perform tasks such as classifying text. All they have to learn is the task at hand, not the nuances of languages.
If a team of experts is able to identify a set of labels in which they are interested, they can label this data in a single shot. The next step is to fine-tune a pre-trained language model, often by just attaching a logistic regression layer to the outputs of the language model,
to recognize each of these labels.

Instead of having to wait for, not to mention pay for, experts to do large-scale retrospective research, fine-tuned pre-trained language models can perform admirably on text classification with, in many cases, only a few hundred labeled examples.
The trained model can then read through and extract targeted text much faster and much cheaper.
Studies have already been done applying this strategy, using three pre-trained neural language models, \texttt{Bio\textunderscore ClinicalBERT}, to extract
seizure frequency from epilepsy clinical notes.\cite{10.1093/jamia/ocac018}

\subsection{Unstructured Text - Quality of Life}
When treating patients with epilepsy, often our principal aim is to target improving quality of life. Seizure frequency is a numeric represenation that has a measurement
that is well-understood and directly impacts quality of life. However, many measurements of quality of life come directly from patient surveys and other more subjective
metrics that can be more difficult to interpret at scale. While often these surveys call for responses on numeric scales, many offer unstructured text as a method for
patients to freely communicate other information relevant to quality of life, or further explain their responses. This information is of exceptional importance, but it
can be difficult to parse through it at scale without modern natural language processing techniques.

Sentiment analysis, the task of extracting and studying subjective sentiment in text and speech data, is yet another field of NLP that has greatly benefited from the
widespread adoption of transformer architectures, particularly in specialized domains. Freeform responses that can be evaluated as positive or negative, or sometimes into more
granular sentiments, can be learned with machine learning methods. It's worth noting that naive methods on sentiment tend to capture some cases well. The words love and hate are
strong signals, but even their simple negation becomes a diluted signal with possible interpretations of sarcasm that become subjective. Add in the many colorful ways that people
write, and we start to see why rule-based text-mining systems for sentiment are so difficult.

Sentiment analysis models using transformer architectures such as RoBERTa aim to project the large-dimensional output vector of the text, and project this semantic representation
into a basic sentiment of two class, positive or negative. Very often a third class is used to represent neutral sentiment. This method should be used when analyzing free-form responses
in patient surveys using NLP models trained on sentiment. While a model trained on medical data is ideal, it's quite impressive how well a model trained on movie reviews will transfer
over to other sentiment tasks.

The same methods for analyzing basic sentiment can be used to label tones, more granular emotional classifications, or other task-specific points of interest.
It is important to have the survey designers, and/or other domain experts, label these targeted semantic
expressions of their patients. Once a model is trained with a set of positive examples of these targets, it can be used on any text response.

The datasets provided by SeizureTracker, as part of their It's Not Just Seizures (INJS) initiative, provides an excellent example of unstructured text responses being
used as a guide to a suggested set of useful labels that can be learned with NLP techniques.

\subsection{Named Entity Recognition and PII}

Named Entity Recognition (NER) is a subtask of information extraction that classifies unstructured text into personal names, locations, time indications, organizations, and other tags.
NER is an important problem in medicine, if nothing else to provide an ability to mask out Personally-Identifiable Information.
If a model, probably a transformer architecture in practice, is able to identify text as a first name, a home address, or a credit card number, then we can anonymize this information to mask information that is PII-specific.

It should be noted that, although some of this text is easy to catch with rule-based approaches and regular expressions, such as a credit card number, other information requires contextual understanding and an ML solution
to achieve the necessary performance.

Spacy, NLTK, and Stanford NLP all provide out-of-the-box NER models that perform well on unstructured text. Let's look at a Spacy example in practice.

\begin{python}
  # Example 4 - Simple PII Anonymizer

  import spacy

  sample = "The patient, John Smith, began feeling " \
           "symptoms at his home in Seattle, and was " \
           "taken to Virginia Mason Medical Center " \
           "for treatment"
  nlp = spacy.load("en_core_web_md")

  doc = nlp(sample)

  for word in doc.ents:
      sample = sample.replace(word.text, word.label_)

  print(sample)
  # The patient, PERSON, began feeling symptoms at his
  # home in GPE, and was taken to ORG for treatment

\end{python}

\subsection{Clinical Trial Matching and Surgical Candidacy}

There has been an emergence of NLP technologies aimed at clinical trial matching. Different patient characteristics provide against eligibility
criteria of available trials, and because much of this data comes in the form of unstructured text, NLP techniques can be used to turn target
creating structures features for clinical trial matching.

Criteria2Query \cite{10.1093/jamia/ocy178} is a hybrid system that uses NLP methods to transform text into eligibility criteria features that can
be extracted with structured queries. The Clinical Trial Parser and accompanying dataset of 3,314 trials\cite{tseo2020information}, made
publicly available from Meta Research, is a system with a similar goal that combines a number of information extraction techniques from NLP to determine
trial level eligibility.

Surgical Candidacy scoring models are yet another example of a classification task wherein recent NLP approaches have been used and evaluated \cite{Wissel2019ProspectiveVO}
in epilepsy research. While this is a very broad introduction into some of the ways that the field has increasingly incorporated computational linguistics in recent years, it is enough
to recognize a common pattern. As a general rule, when evaluating the potential candidacy of an NLP model to your specific task, ask yourself if the following criteria hold true:
\begin{enumerate}
  \item Is there text data in this task that provides signal toward a targeted classification that typically requires human involvement?
  \item Have others in the medical field tried using NLP models on a similar task?
  \item Does the task that requires either substantial amount of time, money, or scarce resources of domain expertise?
  \item Is there a dataset I can use that is in good shape and does my group have full permissions to work with it on the required hardware?
\end{enumerate}

If the answer is yes to any of these questions, it may be worth trying some of these off-the-shelf techniques or even reaching out to a data scientist or machine learning engineer.
If the answer is yes to all four, then it is definitely worth your time.
