\chapterauthor{Author Name}{}
\chapter{Natural Language Processing}
In this chapter we introduce modern NLP libraries, techniques and their applications.
This chapter will focus on deep learning methods and less on computational linguistics that require nuanced knowledge of linguistics.
We explore what it means to represent words and sequences of words with rich numeric representations that are better-suited toward modern computational tasks.
We aim to capture some of these modern fine-tuned representations that are specially catered toward a semantic lexicon for medical language.
We use these representations and aforementioned tools to showcase a modern reference implementation leveraging PyTorch, PyTorch Lightning and the Hugginface Transformers library.
To wrap it all together, we walk through a complete example that highlights best practices that encourage reproducibility and allow for systematic iterative improvements.
\\

\noindent This includes:
\begin{itemize}
\item An introduction into fundamental NLP concepts
\item Bootstrapping techniques to iterate on a dataset in the low-resource setting
\item Storing of a reference dataset in a publicly-accessible location
\item Downloading, caching, loading, splitting, and preprocessing of the data
\item Setting up of a cloud-based GPU workstation (?) (--this might be overkill for now, but keep if we can)
\item VSCode (?)
\item Monitoring the training run:
  \subitem Logging and experiment tracking
  \subitem Learning curves
  \subitem Metrics
\item Hyperparameter tuning, some tricks of the trade
\item Offline evaluation and sanity checking
\end{itemize}
We will keep the discussion focused on SUDEP prediction from electronic medical record (EMR) notes.
Many of the concepts introduced here are very general and are straightforward translations to domains outside of SUDEP prediction, epilepsy, and even NLP.

\subsection{Introduction to Natural Language Processing}

Natural language processing (NLP) is a field of computer science that deals with the extraction, processing, and understanding of human language.
It is known as the field of computer linguistics, and is a subfield of artificial intelligence.
Common NLP tasks include sentence segmentation, tokenization, part-of-speech tagging, named-entity recognition, parsing, question answering, summarization and classification.

How can we teach a computer to perform these tasks?
The first challenge is that computers, at their core, only understand numbers.
So first we need to think about how words can be represented with numbers such that we can perform calculations on it.
The numbers should allow the computer to assign meaning to words and their context with other words around it.

A simple way to achieve this is to define a vocabulary $V$ of words, and assign each word a unique integer $i$.
The word is then represented as a vector $w$ of length $|V|$ with all zeros and a one at index $i$\footnote{This is also known as a one-hot encoding}.
\begin{verbatim}
  have = [1, 0, 0, 0, 0, 0, ... 0]
  a    = [0, 1, 0, 0, 0, 0, ... 0]
  good = [0, 0, 1, 0, 0, 0, ... 0]
  day  = [0, 0, 0, 1, 0, 0, ... 0]
  ...
\end{verbatim}

We could now simply represent a sentence as the sum of the word vectors $S = \sum_j w_j$. This representation is suitable as input for any classification algorithm (such as logistic regression or a decision tree), to make a prediction about our target variable, e.g. whether the sentence is relevant to our epilepsy task.
This simple representation fulfills our requirments but comes with some drawbacks:
\begin{itemize}
    \item We implicitly assume that each word in the sentence is equally important.
    \item Each word is equally similar to every other word (e.g. by taking the euclidian distance between word vectors).
    \item The representation is invariant to reordering of the words.
\end{itemize}
The first point is a problem, because as we add more word vectors together, the sentence reperesentation will converge to the global average and drown out any signal relevant to the specific sentence at hand.
To address this we can instead write a weighted sum $S = \sum_j \lambda_j w_j$, where each word vector is weighted by its importance $\lambda_j$, and there are statistical methods we can use to compute an importance value\footnote{TF-IDF}.
For example a word like \textit{the} is very common and appears in many different contexts.
It is unlikely that there is a lot of signal we can extract from it.
On the other hand, a word like \textit{epilepsy} will be much more rare and specific to a given task, and we would like to raise its importance.
This has the net effect of allowing us to find good representation for larger word sequences.
% One way of defining importance is to compare how often a word occurs in a document compared to the entire corpus.

To address the second point, we will be touching on a very important concept, not just relevant in NLP, but also in the world of ML in general: Embeddings.
An embedding is a representation of a vector space that captures the similarity between the entities we are encoding, where entities can be words, images, e-commerce products, movies, houses, and many other data modalities.
Scientific progress in the field of ML is often directly about finding algorithms that can learn high quality robust embeddings in an efficient and scalable way, and the final performance of a given architecture is largely determined by the quality of the embeddings it learns.

To illustrate this concept, we will be talking briefly about one of the earliest algorithms that has been used to learn embeddings for words: \textit{word2vec}. The idea behind \textit{word2vec} is to set a word into context with the words surrounding it.
For example the word \textit{bank} may appear in the context of words like \textit{account}, \textit{withdrawal}, or \textit{deposit} and vice versa.
We then translate this idea into a training task:
Given the sum of the one-hot encoded word vectors of the words surrounding the target word, map it to the one-hot encoded vector of the target word.

This is a task that can be solved by a neural network with the following setup:
Add a layer $l_1$ that maps from the input dimension (of size $|V|$) to an intermediate dimension of size $h$ and then another layer $l_2$ that maps from the intermediate dimension to the output dimension (of size $|V|$).
Here $h$ is what is called embedding dimension and $h << |V|$.
\begin{figure}
  \includegraphics[width=\linewidth]{chapters/NLP/figures/word2vec.png}
  \caption{Word2vec}
  \label{fig:word2vec}
\end{figure}
While $h$ is generally a hyperparameter that can be optimized at a later stage, a reasonable choice is to use a value according to the rule of thumb:
\begin{equation}
  h \approx 4\sqrt[\leftroot{3} \uproot{3} 4]{|V|}
\end{equation}
For a vocabulary size $|V| = 30000$ this comes to a value of about $50$.
During training, the network will learn to encode the original sparse indicator vectors in a way that preserves the maximum amount of information necessary to predict the target word, while being forced to squeeze the information through the low dimensional bottleneck.
An interesting side effect of this approach is that we can use part of the network to encode a given word in $V$ by using for example the inverse of $l_2$ as a lookup table.
The vectors that we obtain from this lookup table are called word embeddings and they have some very useful properties:
\begin{itemize}
    \item Similar words tend to be close together (e.g. in terms of euclidian distance).
    \item Averaging word embeddings of a sentence will give us a more robust representation than our naive approach.
    \item We can perform some arithmetic, such as adding and subtracting the embedding vectors to traverse the space.
\end{itemize}
\begin{figure}
  \includegraphics[width=\linewidth]{chapters/NLP/figures/king-man+woman.png}
  \caption{Word2vec}
  \label{fig:kingmanwoman}
\end{figure}
These properties also imply that the representions for two sentences will be close together if they are semantically similar, which will make the job of, say, a downstream classifier much more straight forward - it only has to slice the embedding space.
The hard part of understanding the meaning of the sentence is already done.
Since the training data can be generated from just raw text, it is sufficient to train the embeddings once on a very large corpus\footnote{GloVe} and then reuse them, in effect giving us a headstart when building task specific models.

\subsubsection{Transformers}
Until recently building strong NLP systems required deep understanding of language and its structure, as well as large amounts of data, even when relying on pre-trained word embeddings.
The resulting systems were often still brittle.

In recent years some surprisingly powerful architectures and pre-training regimes have emerged that build on the concepts we discussed here, most notably under the name of \textit{Transformers\footnote{BERT, GPT}}, that address the last bullet point of our representaion's shortcomings.

The details of the inner workings are beyond the scope of this tutorial, for now it suffices to know that they are able to learn robust sentence embeddings with a fine grained understanding of syntactic structure, semantics, and general knowledge of the world.
In fact, they can be considered near the level of a human that just knows the English language (or other languages), along with a broad array of factual knowledge about the world (think Wikipedia).
Just like a human, they can be taught specialty knowledge that is relevant to a given task, in ML we would say we \textit{finetune} them.
This paradigm shift has led to a step function improvement in terms of performance and sample efficiency in a wide variety of NLP tasks where Transformers are the defacto state of the art.

In the following sections, we will discuss a broad set of problem statements Transformers are suitable for, along with a set of techniques a practitioner can employ to arrive at robust solutions, even with limited data.

% Where these models to date generally fall short is in terms of logical inference.
