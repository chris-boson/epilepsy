\section{Deep Learning}

Neural networks have revolutionized the field of NLP. The field has seen three main architectures emerge over time: feedforward deep neural networks (DNNs), recurrent neural networks (RNNs), and transformers.

\subsection{Deep Neural Networks}
DNNs are the simplest form of neural networks and are characterized by their feedforward structure, where information flows only in one direction, from input to output.
DNNs were initially used in NLP for tasks such as sentiment analysis, where the goal was to predict a binary class label (e.g., positive or negative) given an input text.
The key advantage of DNNs was their ability to automatically learn complex representations of the input data using multiple hidden layers, allowing for the capture of high-level features that could not be easily hand-engineered.

However, DNNs have several limitations in NLP.
For example, they do not effectively capture the sequential structure of text data. To overcome this, RNNs were introduced.
RNNs are a type of neural network that have loops in their structure, allowing information to flow both forward and backward through the network.
This enables RNNs to model sequential data such as time series or text, by capturing the dependencies between consecutive elements.

Despite the effectiveness of RNNs, they suffer from the vanishing gradient problem.
This occurs when the gradients used to update the network's parameters become extremely small, making it difficult to train the network effectively.
To solve this, the attention mechanism was introduced in the transformer architecture.
Transformers are a type of neural network that is based on self-attention, which allows the network to attend to different parts of the input sequence in parallel.
This allows transformers to effectively capture long-term dependencies in sequential data, without the limitations of the vanishing gradient problem.

Transformers have proven to be highly effective in NLP, and have become the state-of-the-art architecture for many tasks, including machine translation, text classification, and question answering.
This is largely due to the ability of transformers to effectively capture the relationships between different parts of the input sequence, which is critical for NLP tasks where understanding the context and relationships between words is important.


\subsection{Transformers}
There have been significant advancements in ML due to a variety of factors, such as the availability of larger and larger datasets, as well as ever increasing computational power at lower and lower prices, better software tools, libraries and model architectures.
Especially the advent of deep learning (DL), fueled by the emergence of GPUs from about 2012, enabled scaling to many orders of magnitude larger datasets and models.
The common wisdom before DL was that as the number of parameters in a model increases beyond a certain threshold, it tends to overfit the training data, leading to poor generalization when deployed in the real world.
Deep Learning based models empirically do not seem to exhibit this behavior, and instead tend gain performance as they get bigger, albeit more slowly.

Until recently, building strong NLP systems required deep understanding of language and its structure, as well as large amounts of data, even when relying on pre-trained word embeddings - the resulting systems were often still brittle.

In recent years some surprisingly powerful architectures and pre-training regimes have emerged that build on the concepts we have discussed here, most notably under the name of \textit{Transformers\footnote{BERT, GPT}}, that address the last bullet point of our representation's shortcomings.

The details of the inner workings are beyond the scope of this tutorial, for now it suffices to know that they are able to learn robust sentence embeddings with a fine grained understanding of syntactic structure, semantics, and general knowledge of the world.
In fact, they can be considered near the level of a human that just knows the English language (or other languages), along with a broad array of factual knowledge about the world (think Wikipedia).
Just like a human, they can be taught specialty knowledge that is relevant to a given task, in ML we would say we \textit{finetune} them.
% Where these models to date generally fall short is in terms of logical inference.
This paradigm shift has led to a step function improvement in terms of performance and sample efficiency in a wide variety of NLP tasks where Transformers are the defacto state of the art.

In the following sections, we will discuss a broad set of problem statements Transformers are suitable for, along with a set of techniques a practitioner can employ to arrive at robust solutions, even with limited data.

\subsubsection{Pre-training}
Modern DL-based systems, such as the aforementioned Transformer architectures, are pre-trained on gigantic datasets\footnote{BooksCorpus (800M words, Wikipedia 2,500M words)}\cite{bertpaper} and can be downloaded for free\footnote{Huggingface}.
Starting with a model pre-trained on a broad set of topics significantly reduces the amount of task specific training data required to achieve a given performance.
Furthermore, it may be helpful to start with a model that is pre-trained on more domain-specific datasets such as \texttt{BioBERT\cite{DBLP:journals/corr/abs-1901-08746}} or \texttt{Bio\textunderscore ClinicalBERT\cite{clinicalbert}}, or pre-train your model from scratch.

